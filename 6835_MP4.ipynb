{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6835_MP4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goddice/funny-stuffs/blob/master/6835_MP4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIvjJYhtVvbS",
        "colab_type": "text"
      },
      "source": [
        "# 6.835 MiniProject 4: Expression Recognition\n",
        "\n",
        "## Due Monday March 16 at 5pm\n",
        "\n",
        "![alt text](https://i.pinimg.com/originals/56/be/63/56be63d62aa5b174bf9daa13a700a8d2.jpg)\n",
        "Figure 1: Actress Scarlet Johansson making various face expressions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPzFgFEuYt-F",
        "colab_type": "text"
      },
      "source": [
        "## **1\tIntroduction**\n",
        "\n",
        "The goal of this project is to explore the task of expression analysis and emotion classification over two different data sets: 2D images and 3D point clouds. You will implement and compare several neural network architectures, building on what you learned in Mini Project 2. In addition, you’ll get experience with Transfer Learning for creating personalized models. Please start early and ask questions!\n",
        "The data sets you will use in this project are:\n",
        "\n",
        "1) [Kaggle Facial Expression Recognition Challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expressionrecognition-challenge/data)\n",
        "\n",
        "2)\tPersonal face expression images generated by you\n",
        "\n",
        "3)\tStaff generated point clouds from the iPhone X True-Depth Camera Included with this project is iMotions’s “Facial Expression Analysis: The Complete Pocket Guide.” Use this as a reference throughout the project and please go through it before writing any code. It describes the anatomical characteristics of expression and shows examples of each type of expression.\n",
        "\n",
        "The Facial Action Coding System (FACS) is a tool for measuring expressions first published in 1978 by Ekman and Friesen. It is an anatomical system for describing all observable face movement. It breaks down expressions into individual components of muscle movement known as Activation Units. In this section, we will ask you to describe Facial Expressions based on Activation Units from FACS. A complete guide to FACS and AUs can be found on [iMotions blog](https://imotions.com/blog/facial-action-coding-system/) ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK8nMudUedFQ",
        "colab_type": "text"
      },
      "source": [
        "## **2\tGetting Started**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaqQX84_edNp",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Guide to using Colab\n",
        "Colab is a free Jupyter notebook environment by Google that runs entirely in the cloud. It does not require a setup and allows you to run code and see the output immediately (see [here](https://towardsdatascience.com/a-beginners-tutorial-to-jupyter-notebooks-1b2f8705888a) for more info on Jupyter notebooks and [here](https://www.tutorialspoint.com/google_colab/your_first_colab_notebook.htm) for a walkthrough of how Colab works). \n",
        "\n",
        "Colab notebooks are automatically saved on your Google Drive account but can also be saved on a github account. For the purposes of this lab, you only need to know the very basics of using Jupyter notebooks (and by extension Colab).\n",
        "\n",
        "**Coding Interface**\n",
        "*   You will write your code directly into the code blocks in the notebook\n",
        "*   To run your code, you can either press ctrl-enter when inside the cell, or click the run button in the upper left corner of the cell (need to hover over the brackets for it to appear).\n",
        "*   In order for this lab to work correctly, you should run every cell in order (i.e., as you come upon a code cell, even if it's just staff code, please run it). \n",
        "*   If a cell has been run, a number will appear in brackets in the upper left corner where the run button appears. This number helps you track the order of the calls.\n",
        "*   You are welcome to add any new coding blocks you want (by clicking + Code in the top menu) but you cannot import any more libraries than the ones in this project\n",
        "\n",
        "**Uploading files**\n",
        "*   To upload files from your computer into the notebook (will be required later in the project), click the folder icon on the sidebar on the left. The upload button will let you select the file\n",
        "*   File reading works the same as in a normal IDE. You have to specify the path to your file if it is inside a folder vs. in the main file area.\n",
        "*   Everytime Colab is closed (or refreshed), uploaded files are removed and must be re-uploaded.\n",
        "\n",
        "**Saving files**\n",
        "\n",
        "You will be saving some trained models in this mini project to include in your submission. We suggest that each time you save a model, you also download it to your local machine. Saved files are removed if Colab is closed or refreshed, so you may want to download them just in case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9YhOr63B20s",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Colab Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seLlgFqQemMW",
        "colab_type": "text"
      },
      "source": [
        "#### Importing Data\n",
        "We will be pulling in two datasets for this mini project. The Kaggle data set is located in kaggle fer2013/fer2013.csv. There are 28K training and 3K testing images in the dataset, each composed of a 48x48 square of pixels and labeled with an emotion [0-6] (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The iPhone X dataset is located in iPhoneX/faces.py. There are samples of 50 subjects posing with 7 different expressions. Each sample consists of 1220 (x, y, z) points to make up a depth map. We’ll explore this data more in Section 5.\n",
        "\n",
        "The data will be pulled into a folder names mp4_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oVQcgvzrzlP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "aca07e41-713a-4449-9ab7-9a384a35d7c4"
      },
      "source": [
        "! git clone https://gitlab.com/JMadiedo/mp4_data.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mp4_data'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 22 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (22/22), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0VEuwaGdv9V",
        "colab_type": "text"
      },
      "source": [
        "#### Setting up imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcNlTVaGBqPg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4298875e-a7db-4bc7-a303-6e85342a8a02"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3bSyF3gd2aZ",
        "colab_type": "text"
      },
      "source": [
        "#### Predefined Staff Varaibles/Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux5GrGrkNy8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emotions = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "\n",
        "def plot_emotion_prediction(pred):\n",
        "    \"\"\"\n",
        "    Plots the prediction for each emotion on a bar chart\n",
        "    :param pred: the predictions for each emotions\n",
        "    \"\"\"\n",
        "    labels = np.arange(len(emotions))\n",
        "    plt.bar(labels, pred, align='center', alpha=0.5)\n",
        "    plt.xticks(labels, emotions)\n",
        "    plt.ylabel('prediction')\n",
        "    plt.title('emotion')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOfQfKAi2W5_",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Mini Project Structure\n",
        "\n",
        "In this miniproject, you will\n",
        "\n",
        "1) \tCode a Convolutional Neural Network using Keras to detect Face Expressions from a large dataset of images\n",
        "\n",
        "2)\tUse transfer learning to adapt the CNN you’ve created into a personalized network for your own expressions (you’ll have to supply the data)\n",
        "\n",
        "3)\tExplore a new type of available face data - point clouds from the iPhone X True Depth Camera and classify samples by modifying your original CNN.\n",
        "\n",
        "Please do not hesitate to post for help on Piazza using the miniproject4 tag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6XWup6RaSC0",
        "colab_type": "text"
      },
      "source": [
        "## **3\tExpression Recognition with Convolutional Neural Networks**\n",
        "As you read about and saw during the lecture on face recognition, the hidden layers of a Convolutional Neural Network (CNN) typically consist of some combination of convolutional layers, pooling layers, fully connected layers and normalization layers. We’ll give a brief overview of the CNNs and what these layers do. If you don’t have any experience with CNNs, watch [this video](https://www.youtube.com/watch?v=YRhxdVk_sIs) on YouTube to get a solid understanding of how they function.\n",
        "\n",
        "Together, we’ll code up a simple CNN to process the Kaggle dataset. You are encouraged to improve this model by adding additional layers. The Input layer will take in image data (represented as a matrix of numbers) and pass them into a convolution layer. Here the image data is “convolved” - a filter (i.e., a function) is applied methodically to overlapping tiles of the input. The values the filter produces (technically, the dot product of the filter with each sub matrix, is itself another matrix of data. The final layer, the fully connected layer, takes the convolution and produces a vector of predictions.\n",
        "\n",
        "Now it’s time to write a CNN using the Keras API and Tensorflow backend. We’ve already started an implementation for you below. You should complete the implementation by following these steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pes_LpVBpy_",
        "colab_type": "text"
      },
      "source": [
        "###Part A\n",
        "We need to understand our input before we can begin our model. The Kaggle dataset contains 35888 images: 28709 for training and 3589 for testing. Let’s organize this data so we can use it in out model. \n",
        "\n",
        "The supplied code imports the data from the .csv file for you. Each line of data contains an emotion label, image data, and test/train usage. The emotion label is a number between 0 and 6, corresponding to the labels specified above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olVJFVlOyzN9",
        "colab_type": "text"
      },
      "source": [
        "1) We parse the .csv file for you into x train, the training image pixel data as 1D arrays of pixels, y train, the labels corresponding to the training images, x test, the testing image pixel data as 1D arrays of pixels, y test, the labels corresponding to the test images. You will need to do this on your own later in the mini project\n",
        "\n",
        "2)\tThe pixel values in the image data are strings. Convert them to float32 and normalize the inputs to be between 0 and 1.\n",
        "\n",
        "\n",
        "3) Reshape the image data so we can enter samples into our modes with the shape (48, 48, 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvOsTz1xvB6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "\n",
        "def split_data():\n",
        "  # READ IN KAGGLE DATA\n",
        "  with open(\"mp4_data/kaggle_fer2013/fer2013.csv\") as file:\n",
        "      data = file.readlines()\n",
        "\n",
        "  lines = np.array(data)\n",
        "\n",
        "  x_train, y_train, x_test, y_test = [], [], [], []\n",
        "\n",
        "  # A. 1) SPLIT DATA INTO TEST AND TRAIN\n",
        "  for i in range(1,lines.size):\n",
        "      emotion, img, usage = lines[i].split(\",\")\n",
        "      val = img.split(\" \")\n",
        "      pixels = np.array(val, 'float32')\n",
        "      emotion = keras.utils.to_categorical(emotion, num_classes)\n",
        "\n",
        "      if 'Training' in usage:\n",
        "          y_train.append(emotion)\n",
        "          x_train.append(pixels)\n",
        "      elif 'PublicTest' in usage:\n",
        "          y_test.append(emotion)\n",
        "          x_test.append(pixels)\n",
        "\n",
        "  # A. 2) CAST AND NORMALIZE DATA\n",
        "  x_train = np.array(x_train).astype(np.float32)\n",
        "  x_test = np.array(x_test).astype(np.float32)\n",
        "  max_val = np.amax(x_train);\n",
        "  x_train = x_train/max_val\n",
        "  x_test = x_test/max_val\n",
        "\n",
        "  # A. 3) RESHAPE DATA\n",
        "  x_train = x_train.reshape(x_train.shape[0],48,48,1);\n",
        "  x_test = x_test.reshape(x_test.shape[0],48,48,1);\n",
        "\n",
        "  return x_train, y_train, x_test, y_test\n",
        "  \n",
        "# split_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xfYhovltrk2",
        "colab_type": "text"
      },
      "source": [
        "### Part B\n",
        "Now it’s time to code the CNN. We’ll make use of the Keras functional API for building models. For more information on Keras, see the Keras Tutorial online and explore the documentation [here](https://keras.io/getting-started/functional-api-guide/) . We’ll create a simple model together that gives you a working solution. However, we will later expect you to add additional layers to this model to improve its performance.\n",
        "\n",
        "1)\tCreate an Input layer that takes in data of shape (48, 48, 1, ). This is the size of our photos\n",
        "\n",
        "2)\tAdd a Convolutional layer to the network using the 2D convolution layer for spatial convolution over images. Make sure the layer has the following properties: filters=64., kernel_size=(5,5), activation=‘relu’\n",
        "\n",
        "3)\tAdd a MaxPooling2D layer with pool_size=(5,5) and strides=(2, 2)\n",
        "\n",
        "4)\tAdd a Flatten layer which converts the data into a 1D feature vector ready for classification\n",
        "\n",
        "5)\tAdd a Dense layer with 1024 units and activation=‘relu’\n",
        "\n",
        "6)\tAdd a final Dense layer with 7 units (for classification) and the ‘softmax’ activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMDdYDmBtsbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B. CREATE CNN MODEL\n",
        "\n",
        "def create_model():\n",
        "  inputs = Input(shape=(48, 48, 1, ))\n",
        "  conv_layer = Conv2D(filters=64, kernel_size=(5,5), activation=\"relu\")(inputs)\n",
        "  pool_layer = MaxPooling2D(pool_size=(5,5), strides=(2,2))(conv_layer)\n",
        "  flatten_layer = Flatten()(pool_layer)\n",
        "  dense_layer = Dense(1024, activation=\"relu\")(flatten_layer)\n",
        "  outputs = Dense(7, activation=\"softmax\")(dense_layer)\n",
        "  # INSERT LAYERS HERE\n",
        "  return Model(inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa2syoyZuCvR",
        "colab_type": "text"
      },
      "source": [
        "### Part C\n",
        "Now it’s time to train our model and see how well it performs.\n",
        "\n",
        "1) Batch the training and testing data using the Keras ImageDataGenerator() with .flow(..., batch  size = 256). The ImageDataGenerator does image augmentation and artificially creates training images through different ways of processing or combination of multiple processing, such as random rotation, shifts, shear and flips, etc. Here we are using it to randomly selected training set instances for our model.\n",
        "\n",
        "2) Compile your model with loss=‘categorical_crossentropy’ and the Adam optimizer (i.e. keras.optimizers.Adam()).\n",
        "\n",
        "3)  Train your model by calling model.fit generator(...) and the provided steps per epoch = len(x train)/batch size and epochs = 5 variables. Make sure to save your model after training it: export the model to a .h5 file using the built in model.save(‘model 1.h5’) (please use this naming convention). The model should take about 10 minutes to run and should achieve about 55% accuracy.\n",
        "\n",
        "**Note: you may want to download the model to your local machine just incase Colab crashes or unexpectedly closes. You can download by right-clicking on the model in the file sidebar and selecting download**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5zNiXpSckCf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "aae12d24-78c4-4a53-b0af-186926047fd3"
      },
      "source": [
        "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "\n",
        "def cnn():\n",
        "  x_train, y_train, x_test, y_test = split_data()\n",
        "  model = create_model()\n",
        "\n",
        "  # C. 1) DATA BATCH PROCESS\n",
        "  data_gen = ImageDataGenerator().flow(x_train, y_train, batch_size=batch_size)\n",
        "\n",
        "  # C. 2) COMPILE MODEL\n",
        "  model.compile(loss='categorical_crossentropy', \n",
        "                optimizer='adam', \n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  # C. 3) TRAIN AND SAVE MODEL\n",
        "  model.fit_generator(data_gen, steps_per_epoch=len(x_train)/batch_size, epochs=5)\n",
        "\n",
        "  model.save('model_1.h5')\n",
        "\n",
        "cnn()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " 83/112 [=====================>........] - ETA: 52s - loss: 2.0071 - acc: 0.2804"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_a-5wUIassw",
        "colab_type": "text"
      },
      "source": [
        "4) You can test your model with the script below to see how a certain example (jackman.png) gets classified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rslpj5PB2Wdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_cnn():\n",
        "  model = load_model('my_model_1.h5')\n",
        "\n",
        "  img = image.load_img(\"mp4_data/jackman.png\", color_mode = \"grayscale\", target_size=(48, 48))\n",
        "\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "  x /= 255\n",
        "\n",
        "  custom = model.predict(x)\n",
        "  plot_emotion_prediction(custom[0])\n",
        "\n",
        "test_cnn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsrwcrMCmJeB",
        "colab_type": "text"
      },
      "source": [
        "### Part D\n",
        "\n",
        "Now, modify the model to improve your accuracy. You may change the parameters (such as batch size) and layers of the model. We've provided new code blocks below for you to experiment in (you can copy over your code from part C).\n",
        "\n",
        " In your writeup, include:\n",
        "\n",
        "*   A diagram of your final network architecture\n",
        "*   A description of the structure and the parameters you used.\n",
        "*   The accuracy and loss for your model\n",
        "\n",
        "In the original Kaggle challenge, the winner achieved just 34% accuracy - so congrats, your model is already much better! Be sure to save your trained model as model_2.h5, i.e., following our naming convention. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-J3qUpc4PRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "\n",
        "def create_model():\n",
        "  inputs = Input(shape=(48, 48, 1, ))\n",
        "  # INSERT LAYERS HERE\n",
        "  return Model(inputs, outputs)\n",
        "\n",
        "def cnn():\n",
        "  x_train, y_train, x_test, y_test = split_data()\n",
        "  model = create_model()\n",
        "\n",
        "  # C. 1) DATA BATCH PROCESS\n",
        "\n",
        "  # C. 2) COMPILE MODEL\n",
        "\n",
        "  # C. 3) TRAIN AND SAVE \n",
        "  \n",
        "  # model.save('model_2.h5')\n",
        "  \n",
        "cnn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qfv0PTD4lQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_cnn():\n",
        "  model = load_model('my_model_2.h5')\n",
        "\n",
        "  img = image.load_img(\"mp4_data/jackman.png\", color_mode = \"grayscale\", target_size=(48, 48))\n",
        "\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "  x /= 255\n",
        "\n",
        "  custom = model.predict(x)\n",
        "  plot_emotion_prediction(custom[0])\n",
        "  \n",
        "test_cnn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7V-cFtlbwZ8",
        "colab_type": "text"
      },
      "source": [
        "##4\tTransfer Learning for a Personalized Machine Learning Model\n",
        "\n",
        "In practice, very few people train an entire Convolutional Network from scratch (with random initialization) as we just did. This is because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pre-train a CNN on a (different) very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the CNN either as an initialization or a fixed feature extractor for the task of interest.\n",
        "\n",
        "The process of sharing results across different problems is known as Transfer Learning. In other words, Transfer Learning is a machine learning technique where a model trained on one task is re-purposed for a second related task.\n",
        "\n",
        "In the section, we’ll first ask you to generate your own expression data (yes, you’ll have to take pictures of yourself) and use that data to fine-tune your CNN from the previous section into a personalized model for your face, using Transfer Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emQCJU-0RNuu",
        "colab_type": "text"
      },
      "source": [
        "###Part E\n",
        "Pose in front of a camera or get a friend to help you take 7 pictures: one for each type of expression: angry, disgust, fear, happy, sad, surprise, neutral. This is your training data. Crop these images so that they contain a bounding box of only your face. It is OK if the pictures are not 48 x 48, Keras will resize them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO1AgNsxRP_y",
        "colab_type": "text"
      },
      "source": [
        "###Part F\n",
        "Take 7 more pictures. This is your testing data. \n",
        "\n",
        "Add your training and testing images to the notebook:\n",
        "\n",
        "1) Open the files section of the sidebar on the left side of your screen\n",
        "\n",
        "2) Right-click in the area to open the context menu. Select create new folder.\n",
        "\n",
        "3) Upload your images (you can upload multiple at a time)\n",
        "\n",
        "4) Structure the folder and name your images as you like"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdLjc4zLRSGp",
        "colab_type": "text"
      },
      "source": [
        "###Part G\n",
        "Now it’s time to load your model from the previous section. We’ve started an implementation for you below.\n",
        "\n",
        "1) Import the data from your images and reshape the data so that you can retrain your model from Section 3 (model 2.h5). You will need to grayscale your images. You may find functions in keras.preprocessing useful for image manipulation.\n",
        "\n",
        "2) Load your model using load_model(‘model_2.h5’) and train the model on your 7 training images.\n",
        "\n",
        "3) Finally, test the newly trained model on your test images. Save your trained model as model_3.h5. Remember we provided the helper function plot_emotion_prediction(pred) at the beginning of this notebook that takes in a model prediction values from a single call of model.predict(x) and plots them on a bar graph. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VLRuqqw1mB4g",
        "colab": {}
      },
      "source": [
        "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
        "batch_size = 7\n",
        "epochs = 3\n",
        "\n",
        "def transfer():\n",
        "  # G. 1) Import data and reshape data\n",
        "  \n",
        "  # G. 2) Load model and train on training images\n",
        "\n",
        "  # G. 3) Test newly trained model and save\n",
        "\n",
        "transfer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8iIYdQHAyOb",
        "colab_type": "text"
      },
      "source": [
        "4)  Which expressions are not being recognized? Why do you think some expressions are recognized better than others? Report the accuracy of the model.\n",
        "\n",
        "5) If your model did not achieve good accuracy on your personal data, explain why you think that is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCvEX_msnO3J",
        "colab_type": "text"
      },
      "source": [
        "## 5\tFrom Images to Depth: Next Generation of Face Representation\n",
        "In this section, we expect you to implement a fully functional model on your own. Your work in this part will be graded on correctness, not on how accurate your final model is. \n",
        "\n",
        "Recall structured light: the technology used by the Xbox Kinect that shines thousands of infrared dots in an area and can create a corresponding depth map. The iPhone X also uses this technology to scan a user’s face. We’ve accumulated samples of 100 iPhone X users posing with different face expressions using [Apple’s ARKit framework](https://developer.apple.com/documentation/arkit/arfaceanchor?language=objc) . Apple anchors the face into a specific origin, and provides us with vertex positions for each point in the face mesh. These vertices are referred to as a point cloud. The point cloud we receive is sparse, so we see a smoothed version of an actual face. Together, this normalizes all of our data and makes it ready for analysis.\n",
        "\n",
        "![alt text](https://www.bing.com/th?id=OIP.nxQY5PjyWSCBU1oYGGqy3AHaDS&pid=Api&rs=1)\n",
        "\n",
        "Figure 2: Origin of the face coordinate system.\n",
        "\n",
        "\n",
        "In this last part, we’ll classify the 3D data into the same 7 emotions: angry, disgust, fear, happy, sad, surprise, neutral. However, this time we’re expecting you to craft the model. The model you will create should be a 3D Convolutional Neural Network. They are the same in essence to 2D CNNs, but perform operations in 3 dimensions. In order to use a 3D CNN, we’ll have to transform our input data into voxels. Voxels are the three-dimensional analogue of a pixel:\n",
        "unit volumes of space that contain a value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB7Tu2yxnb2_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Part H\n",
        "To visualize the various expressions, you will need to use the Visualization GUI we provided in the zip file for you to run on your local machine. To use the GUI, run\n",
        "\n",
        "```\n",
        "python show_gui.py\n",
        "```\n",
        "\n",
        "\n",
        "You’ll be able to see the 7 different expressions. You can drag the graph to view the data from different orientations. \n",
        "\n",
        "\n",
        "\n",
        "1) Inspect the 3D face data and give us your impressions. Compare the expression data using FACS. Which expressions are the most unique? Which expressions are most similar? What information does the point cloud provide us that the image does not?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N94tERYLiiZL",
        "colab_type": "text"
      },
      "source": [
        "Now it's time to try your hand at making your own model here in Colab! The data is provided as a Python dictionary \n",
        "\n",
        "```\n",
        "face_samples = { sample id : { emotion: { x: [...], y: [...], z: [...]}}}\n",
        "```\n",
        "\n",
        "2)  Read in the iPhone X data. For each point cloud, create a 24 x 24 x 24 voxel grid represented as a 3D numpy array initialized with all 0s. For each point in the cloud, increment the value of the voxel that the point falls in.\n",
        "\n",
        "3) Construct a 3D Convolution Neural Network using Keras. You can use your previous work as a starting point, but will have to make use of Conv3D and MaxPool3D from Keras. You are free to add as many layers as you’d like.\n",
        "\n",
        "4) Train and test your model. Save your trained model as model_4.h5.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPduEIeGiC-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
        "batch_size = 12\n",
        "epochs = 20\n",
        "\n",
        "emotions = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "\n",
        "def cnnX():\n",
        "  # H. 2) READ IN iPHONE X DATA AND SHAPE\n",
        "\n",
        "  # H. 3) CREATE MODEL OF CHOICE\n",
        "\n",
        "  # H. 4) TRAIN AND TEST MODEL. SAVE AS model_4.h5\n",
        "\n",
        "cnnX()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEsXqNUko1bD",
        "colab_type": "text"
      },
      "source": [
        "5) In your writeup, include:\n",
        "\n",
        "*   A diagram of your final network architecture\n",
        "*   A description of the structure \n",
        "*   The test/train ratio\n",
        "*   The parameters you used (batch size, number of epochs)\n",
        "*   The accuracy and loss for your model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1OG6-_Qe3y",
        "colab_type": "text"
      },
      "source": [
        "## Extra Credit\n",
        "###Extra Credit 1 (5 points): \n",
        "Continue to modify your model until you achieve an accuracy >= 42%. Save your model as model_ec1.h5.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5znD9JC1hGj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EC. 1) Accuracy >= 42%"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csw2E6B8hGvb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "###Extra Credit 2 (10 points maximum): \n",
        "Use a different classification technique to classify the data (it does not have to be deep learning). Describe what you built and report how well your classification works. Be sure to include where you found inspiration for the implementation and what additional libraries you used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ9-8FJ7RAn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EC. 2) Different Classification Technique"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U61dRxyKQzyT",
        "colab_type": "text"
      },
      "source": [
        "## Exporting your Colab Notebook for Submission\n",
        "\n",
        "Once you're done implmenting all the parts of the project, you will need to download your Colab Notebook and models to include as part of your submission.\n",
        "\n",
        "1) For each model you saved, download it by right-clicking on it in the files and selecting download.\n",
        "\n",
        "2) To download the notebook, click *download .ipynb* in the file menu.\n"
      ]
    }
  ]
}